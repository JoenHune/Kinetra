# Kinetra Agent Configuration
# ─────────────────────────────

project:
  root: ".."                          # Relative to agent/ dir
  build_dir: "../build"
  cmake_preset: "default"
  cmake_options:
    - "-DCMAKE_BUILD_TYPE=Release"
    - "-DKINETRA_BUILD_TESTS=ON"
    - "-DKINETRA_BUILD_BENCHMARKS=ON"

agent:
  max_iterations: 20
  auto_commit: false                  # Git commit after each green iteration
  log_dir: "logs"
  report_dir: "reports"

# ── LLM Configuration ────────────────────────────────────────────────────────
# The agent uses an OpenAI-compatible API for Design and Implement phases.
# Supported providers: openai, anthropic, deepseek, ollama, or any custom
# endpoint that speaks the OpenAI chat-completions protocol.
#
# WARNING: DO NOT put api_key / api_base here — they will be pushed to remote!
#   Create  config.local.yaml  (git-ignored) to store secrets:
#     cp config.local.yaml.example config.local.yaml
#
# API key resolution order:
#   1. config.local.yaml  -> llm.api_key  (recommended)
#   2. config.yaml        -> llm.api_key  (NOT recommended, will be pushed)
#   3. Environment variable: OPENAI_API_KEY / ANTHROPIC_API_KEY / DEEPSEEK_API_KEY
#   4. LLM_API_KEY (generic fallback)
#
# To run without LLM (CI-only mode): set enabled: false or use --no-llm flag
llm:
  enabled: true
  provider: "deepseek"                # openai | anthropic | deepseek | ollama | custom
  model: "deepseek-chat"              # Model name
  # api_base and api_key: set in config.local.yaml or env vars
  temperature: 0.3                    # Lower = more deterministic
  max_tokens: 8192                    # Max response tokens
  timeout: 120                        # Request timeout (seconds)

build:
  parallel_jobs: 0                    # 0 = auto-detect
  timeout_seconds: 120
  sanitizers: false                   # Enable ASan/TSan during test phase

test:
  timeout_seconds: 300
  retry_on_failure: 1
  coverage: false                     # Generate lcov coverage (slower)

benchmark:
  timeout_seconds: 600
  repetitions: 3
  regression_threshold_pct: 5.0       # Flag if >5% slower than baseline
  baseline_file: "logs/benchmark_baseline.json"

research:
  enabled: true
  max_queries_per_iteration: 3
  sources:
    - "arxiv"
    - "github"
    - "scholar"
  cache_dir: "logs/research_cache"

reflect:
  knowledge_base: "logs/knowledge.jsonl"   # Append-only learnings
  focus_areas:
    - "trajectory_optimization"
    - "sampling_based_planning"
    - "collision_checking"
    - "embedded_performance"
    - "numerical_stability"
